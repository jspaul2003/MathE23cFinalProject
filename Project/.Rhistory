nrow(data2)
#ANALYSIS
#PART 1:
#Exploration regarding Distribution of Variables related to deathrate from COVID-19
#using probability distributions.
#I)
#Can we model the time series data of new deaths world wide as binomial or poisson distribution?
#This would make sense as each case could be considered a bernoulli variable with probability of
#surviving or dying.
#We will look at the interval between days 1 to 106. We will use the more in depth
#in deathrate, geographicdata dataframe. It will contain 55 countries with data on these days.
#(REQ: A barplot,Nicely labeled graphics using ggplot, with good use of color, line
#styles...)
#max days
max(geographicdata$days)
#world will be a culmination of all the countries we have data on, with data on
#total deaths and cases, daily deaths amd mew cases, active cases, deathrate
#(deathrate will be data on all available countries not just the 55 as this should
#make it more accurate without damaging analysis)
world=geographicdata[1:max(geographicdata$days),]
world$deathrate=1
a=subset(geographicdata,geographicdata$days==1)
logic=geographicdata$days==max(geographicdata$days)&geographicdata$countriesAndTerritories%in%a$countriesAndTerritories
a=subset(geographicdata,logic)
#number of countries at days 1 and last day
nrow(a)
for(i in 1:(max(geographicdata$days))){
index=which(geographicdata$days==i&(geographicdata$countriesAndTerritories%in%a$countriesAndTerritories))
index2=which(geographicdata$days==i)
world$Date[i]=i+min(geographicdata$Date)
world$days[i]=i
world$deathrate[i]=sum(geographicdata$deaths2[index2])/(sum(geographicdata$cases2[index2])+1*(sum(geographicdata$cases2[index2])==0))
world$deaths2[i]=sum(geographicdata$deaths2[index])
world$cases2[i]=sum(geographicdata$cases2[index])
world$deaths[i]=sum(geographicdata$deaths[index])
world$cases[i]=sum(geographicdata$cases[index])
world$active[i]=sum(geographicdata$active[index])
}
world$days=as.numeric(world$days)
#Could we plot a binomial distribution for deaths
model=world$active*0.17*pbinom(1:nrow(world),(round(world$active/1000)),mean(world$deathrate))
ggbar(world$deaths2,model)
#Seems like a weak model.
#How about a poisson model?
#we can use fitdistr to give us our parameters for the poisson distribution
fitdistr(world$deaths2, "poisson")
model=world$active*0.17*ppois(1:nrow(world), 17.83948)
ggbar(world$deaths2,model)
#Seems just as weak as the binomial model.
#II)
#How are death rates distributed: do they converge at a certain value or do they vary wildly?
#(REQ: A histogram, A probability density function overlaid on a histogram, A p-value or other
#statistic based on a distribution function, Appropriate use of R functions for a probability
#distribution other than binomial, normal, or chi-square, Nicely labeled graphics using ggplot,
#with good use of color, line styles...)
toplot=subset(geographicdata, deaths2!=0)
p=ggplot(toplot, aes(x=deathrate))  +
geom_histogram(aes(y=..density..), binwidth=0.01, colour="steelblue", fill="white",bins="fd") +
ylab("Density")+xlab("Death Rate")+ggtitle("Density Histogram of death rate")
p
#Lets try modelling with a gamma function
#we can use fitdistr to give us our parameters for the gamma distribution
fitdistr(geographicdata$deathrate[which(geographicdata$deaths2!=0)], "gamma")
p+stat_function(fun=dgamma, args=list(shape=1.06011822, rate=21.99330112),col="red")+xlim(0,1)
#Seems good
#Lets test if this is a good model
#create bins by breaking data into deciles
bins=qgamma(0.1*(0:10),1.06011822 ,21.99330112)
binstuff=cut(geographicdata$deathrate[which(geographicdata$deaths2!=0)], breaks=bins,labels=F); binstuff
obs=as.vector(table(binstuff))
exp=rep(sum(obs)/10,10)
chisq=ChiSq(obs,exp); chisq
#P-value
#10 Categories, imposed 2 parameters, set total of actual and expected equal. Therefore we
#lose 3 dfs to get 7dfs
pval=pchisq(chisq,df=7,lower.tail = F); pval
#We got a p-value of 1.651946e-37. It is thus extremely improbable that we observe a test
#statistic this extreme from the relevant chi square distribution, so under the current
#evidence we strongly reject the null hypothesis at the 0.05 level of significance that the
#data follows a gamma distribution.
#PART 2:
#Exploring Correlations and Independences within our dataframes to Deathrate
#These could be use later on for modelling.
#I)
#Is there a significant difference in death rates between rich countries
#and poor countries?
#We will check via a permutation test and t test to confirm.
#(REQ: A permutation test, Comparison of analysis by classical methods (chi-square,
#CLT) and simulation methods, An example where permutation tests or other computational
#techniques clearly work better than classical methods Nicely labeled graphics using
#ggplot, with good use of color, line styles...)
#we will look only at the latest data from the latest date; as such trying to avoid cases
#where countries are only at the start of the outbreak, and repetition of countries whose
#death rate may not change much whereas gdp/capita in this data is held fixed
data2$Date[which.max(data2$Date)]
rich=which(data2$Rich==1&data2$Date==data2$Date[which.max(data2$Date)])
poor=which(data2$Rich==0&data2$Date==data2$Date[which.max(data2$Date)])
deathRateDif=mean(data2$deathrate[rich])-mean(data2$deathrate[poor]); deathRateDif
#Unsuprisingly Higher, but not by much. Likely to be insignificant.
temp=data2[which(data2$Date==data2$Date[which.max(data2$Date)]),]
N=100000; diff=numeric(N)
for(i in 1:N){
samp=sample(nrow(temp),sum(temp$Rich==0))
drsamp=mean(temp$deathrate[samp])
drother=mean(temp$deathrate[-samp])
diff[i]=drsamp-drother
}
p=ggplot() +
aes(diff)+
geom_histogram(binwidth=0.001,bins=100, colour=rgb(0,0.8,0.107,1), fill=rgb(0,0.62,0.107,1))+
ylab("Count")+
xlab("Differences in death rates between rich and poor countries")+
ggtitle("Histogram of differences in death rates between rich and poor countries")
p+geom_vline(xintercept =deathRateDif,col="blue")
#Does not look significant, reaffirming what we thought before
pv.1t=(sum(diff>=deathRateDif)+1)/(N+1)
pv.2t=2*pv.1t;pv.2t
#Not significant at 0.05 level of significance somewhat suprisingly-
#There is a 58.88741% chance of discrepency by chance.
#This is probably because poorer countries do not have the resources
#to test everyone and thus do not know exactly how some might have died.
#Moreover, it is possible that these countries are not tracking all deaths
#in them.
#How about using a t test?
#We will use a two-sample t-test to check whether there is evidence against the
#null hypothesis that two population means are equal
t.test(data2$deathrate[rich],data2$deathrate[poor])
#same conclusion at the 0.05 level of significance, albeit weaker with
#p-value 0.04360523, with us rejecting the null hypothesis of no mean
#difference
#However the weakness could be explained by whether the deathrate follows
#a normal distribution. (Which it does not as shown in I)). We will plot
#again to confirm, and run a shapiro wilk test
p=ggplot(data2, aes(x=deathrate))  +
geom_histogram(aes(y=..density..), binwidth=0.01, colour="steelblue", fill="white",bins="fd")
p+stat_function(fun=dnorm, args=list(mean=mean(data2$deathrate), sd=sd(data2$deathrate)),col="red")+xlim(0,1)
shapiro.test(data2$deathrate)
#we receive p-value 2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, death rate does not follow a normal distribution
#As such the t test is fundamentally flawed. And the permutation test's
#result of a strong rejection is given much more weight. This method is
#more appropriate than the classical method.
#II)
#Are crime and death rates are independent?
#hdr is a logical variable which is 1 for countries in the upper half
#of death rates, and 0 for countries in the lower half.
#We will check via a chi-squared test on hdr and crime.
#(REQ: A contingency table, Analysis of a contingency table)
data2$hdr=0+(data2$deathrate>median(data2$deathrate))
lookingat=data2[which(data2$deaths2>0),]
#cro from expss generates a contingency table
cro(data2$hdr,data2$Crime)
#Run Chi-square test
Observed=table(data2$hdr,data2$Crime)
Expected <- outer(rowSums(Observed), colSums(Observed))/sum(Observed)
chisq <- ChiSq(Observed,Expected); chisq
pval= 1 - pchisq(chisq,1); pval
#We strongly reject null hypothosis of independence at the 0.05 level of
#significance, with a P-value so small that the computer rounded it to 0,
#(much less than 0.05). This looks promising for modelling later on.
#III)
#Exploring Correlation with all our other data variables
#We will carry this out using a correlation heatmap
#(REQ: A graphical display different from those in the textbook
#or in the class scripts, Appropriate use of correlation, Nicely labeled
#graphics using ggplot, with good use of color, line styles...)
#extract numeric columns, ensure everything is numeric
temp <- subset(data2, select = -c(Country,countriesAndTerritories, geoId, countryterritoryCode, continentExp, Date, day, month, year, hdr, Entity, Code, dateRep))
sapply(temp, is.numeric)
temp$days=as.numeric(temp$days)
cormat <- round(cor(temp),2)
# Melt the correlation matrix
upper_tri <- get_upper_tri(cormat)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Heatmap
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
geom_tile(color = "white")+
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Pearson\nCorrelation") +
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, vjust = 1,
size = 10, hjust = 1))+
xlab("")+ylab("")+ggtitle("Correlation heatmap for COVID-19 data")
coord_fixed()
#Death rate actually isnt strongly correlated with much at all,
#with the other variables. Interestingly, we dont see high degrees of
#similarity. Perhaps this indicates that deathrate is relatively constant.
#PART 3:
#Modelling deathrate and deathtoll.
#I)
#Modelling deathrate via logistic regression.
#lets train some models on a random sample of 30% of the data
#and then test on the rest of the sample.
#then we can compare fitted SSE to determine best model
#(REQ: Logistic Regression)
temp <- subset(data2, select = -c(Country,countriesAndTerritories, geoId, countryterritoryCode, continentExp, Date, day, month, year, hdr, Entity, Code, dateRep))
#would be unfair to feed death data here
temp2=subset(temp,select=-c(deaths,deaths2,active))
boxplot(temp2)
#there seems to be major spread of outliers and general spread in
#GDP.2018. We will log(GDP.2018) to improve this.
temp2$GDP.2018=I(log(temp2$GDP))
boxplot(temp2)
#there seems to be major spread of outliers and general spread in
#Population.2020, Total.tests. We will log these to improve this.
temp2$Population.2020=I(log(temp2$Population.2020))
temp2$Total.tests=I(log(temp2$Total.tests))
boxplot(temp2)
#there seems to be major spread of outliers and general spread in
#Cases2, recoveries. We cannot log these as they have 0 values.
#lets try squarerooting these
temp2$recoveries=I(sqrt(temp2$recoveries))
temp2$cases2=I(sqrt(temp2$cases2))
boxplot(temp2)
#there seems to be major spread of outliers and general spread in
#cases. We cannot log this as it has 0 values. lets try
#squarerooting these
temp2$cases=I(sqrt(temp2$cases))
boxplot(temp2)
#80/20 split for training and testing data
sample <-sample.int(n =nrow(temp2),size =floor(.80*nrow(temp2)), replace = F)
train.data <- temp2[sample, ]
test.data <-temp2[-sample,]
train.data=drop_na(train.data)
fit=glm(deathrate~.,data=train.data,family="binomial")
#Check for multicollinearity
vif(fit)
#Deal with multicollinearity
length=1
while(length!=ncol(train.data)){
length=ncol(train.data)
train.data=dropvif1(fit,train.data)
fit=glm(deathrate~.,data=train.data,family="binomial")
}
vif(fit)
#Male.lung seems quite high; although not 10 its very close to it. I will drop
#this as such
train.data=subset(train.data,select = -c(Male.Lung))
fit=glm(deathrate~.,data=train.data,family="binomial")
vif(fit)
#Seems good-we now have our first model
summary(fit)
plot(fit)
#R2
#model 2
fit2=glm(deathrate~.^2,data=train.data,family="binomial"); summary(fit4)
plot(fit2)
#model 3
fit3=stepwise(fit,direction="backward/forward",criterion="AIC", trace=F)
plot(fit3)
#model 4
fit4=stepwise(fit,direction="backward/forward",criterion="BIC", trace= F)
plot(fit4)
#model 5, LASSO Model
X = model.matrix(deathrate~.,train.data )[,-1]
Y=train.data$deathrate
cv=cv.glmnet(X,Y,alpha =1)
model=glmnet(X,Y,alpha =1, lambda=cv$lambda.min)
X1=cbind(1,X)
testlassoridge(X1%*%coef(model));
#plot doesent look random
#we received p-value <2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, the residuals do not follow a normal distribution
#model 6, Ridge Model
cv=cv.glmnet(X,Y,alpha =0)
model2=glmnet(X,Y,alpha =0, lambda=cv$lambda.min)
testlassoridge(X1%*%coef(model2))
#doesent look random on bottom left hand side
#we received p-value <2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, the residuals do not follow a normal distribution
#model 7, neural net
#we need to normalize the data
train.data2=subset(train.data,select=-c(days)) #otherwise get errors when normalizing
ntrain.data=as.data.frame(lapply(train.data2,normalize))
ntrain.data=drop_na(ntrain.data)
nn=neuralnet(deathrate ~ ., data=ntrain.data, hidden=c(16,25), linear.output =T, threshold=0.1)
#model 8, the constant
fit8=mean(train.data$deathrate)
#Now Lets test accuracy!
#We will look at SSE
#we also need to remove the columns in testing data (for testing lasso and ridge)
#that we removed in training
test.data=test.data[which(colnames(test.data)%in%colnames(train.data))]
sse1=sum(test.data$deathrate-predict(fit,new=test.data))^2
sse2=sum(test.data$deathrate-predict(fit2,new=test.data))^2
sse3=sum(test.data$deathrate-predict(fit3,new=test.data))^2
sse4=sum(test.data$deathrate-predict(fit4,new=test.data))^2
X.test = model.matrix(deathrate~.,test.data )
fits = X.test%*%coef(model)
sse5=sum(test.data$deathrate-fits)^2
fits2 =X.test%*%coef(model2)
sse6=sum(test.data$deathrate-fits2)^2
nn.results <- compute(nn, test.data)
sse7=sum(test.data$deathrate-nn.results$net.result)^2
sse8=sum(test.data$deathrat-fit8)^2
c(mean(sse1),mean(sse2),mean(sse3),mean(sse4),mean(sse5),mean(sse6),mean(sse7),mean(sse8))/(nrow(test.data))
r2 <- rSquared(test.data$deathrate, resid = test.data$deathrate-(X.test%*%coef(model2)))
test.data$deathrate-fits2
r2 <- rSquared(test.data$deathrate, resid = test.data$deathrate-(X.test%*%coef(model2))[1:length(sse6)])
r2
X.test%*%coef(model2))[1:length(sse6)]
test.data$deathrate-(X.test%*%coef(model2))[1:length(sse6)]
r2 <- rSquared(test.data$deathrate, resid = test.data$deathrate-(X.test%*%coef(model2))[1:length(sse6)])
r2
r2 <- rSquared(test.data$deathrate, resid = sum(test.data$deathrat-fit8))
r2
r2 <- rSquared(test.data$deathrate, resid = test.data$deathrate-fits2)
sse6
r2 <- rSquared(test.data$deathrate, resid = as.matrix(test.data$deathrate-fits2))
r2
coef(model)
nrow(coef(model2))-1
1-((1-r2)*(n-1))/(n-k-1)
n=nrow(test.data)
k=nrow(coef(model2))-1
1-((1-r2)*(n-1))/(n-k-1)
#II)
#Can we do a better job modelling the total deaths?
#(REQ: Use of linear regression)
temp2=subset(temp,select=-c(deaths2,deathrate,active))
#we can run same code to deal with spread of outliers
temp2$GDP.2018=I(log(temp2$GDP))
temp2$Population.2020=I(log(temp2$Population.2020))
temp2$Total.tests=I(log(temp2$Total.tests))
temp2$recoveries=I(sqrt(temp2$recoveries))
temp2$cases2=I(sqrt(temp2$cases2))
temp2$cases=I(sqrt(temp2$cases))
boxplot(temp2)
#80/20 split for training and testing data
sample <-sample.int(n =nrow(temp2),size =floor(.80*nrow(temp2)), replace = F)
train.data <- temp2[sample, ]
test.data <-temp2[-sample,]
train.data=drop_na(train.data)
fit=lm(deaths~.,data=train.data)
vif(fit)
#deal with multicollinearity
length=1
while(length!=ncol(train.data)){
length=ncol(train.data)
train.data=dropvif1(fit,train.data)
fit=lm(deaths~.,data=train.data)
}
vif(fit)
#looking good, we have our first model
summary(fit)
plot(fit)
fit2=stepwise(fit,direction='forward/backward',criterion='BIC',trace='false'); summary(fit2)
plot(fit2)
fit3=stepwise(fit,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit3)
fit4=lm(deaths~.^2,data=train.data)
plot(fit4)
#model 5
fit5=stepwise(fit4,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit5)
fit6=stepwise(fit4,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit6)
#model 7, LASSO Model
X = model.matrix(deaths~.,train.data )[,-1]
Y=train.data$deaths
cv=cv.glmnet(X,Y,alpha =1)
model=glmnet(X,Y,alpha =1, lambda=cv$lambda.min)
#model 8, Ridge Model
cv=cv.glmnet(X,Y,alpha =0)
model2=glmnet(X,Y,alpha =0, lambda=cv$lambda.min)
#model 9, neural net
#we need to normalize the data
ntrain.data=as.data.frame(lapply(train.data,normalize))
nn=neuralnet(deaths ~ .-cases , data=ntrain.data, hidden=c(16,25), linear.output =T, threshold=0.1)
r2 <- rSquared(test.data$deathrate, resid = test.data$deathrate-(X.test%*%coef(model2))[1:length(sse6)])
#pretty bad 0.4728446 R^2; not accounting for adjusted R^2 considering we
#have many variables
#adjusted R^2
n=nrow(test.data)
k=nrow(coef(model2))-1
1-((1-r2)*(n-1))/(n-k-1)
#about the same, still not great though
test.data=test.data[which(colnames(test.data)%in%colnames(train.data))]
sse1=sum(test.data$deaths-predict(fit,new=test.data))^2
sse2=sum(test.data$deaths-predict(fit2,new=test.data))^2
sse3=sum(test.data$deaths-predict(fit3,new=test.data))^2
sse4=sum(test.data$deaths-predict(fit4,new=test.data))^2
sse5=sum(test.data$deaths-predict(fit5,new=test.data))^2
sse6=sum(test.data$deaths-predict(fit6,new=test.data))^2
X.test = model.matrix(deaths~.,test.data )
fits = X.test%*%coef(model)
sse7=sum(test.data$deaths-fits)^2
fits2 =X.test%*%coef(model2)
sse8=sum(test.data$deaths-fits2)^2
nn.results <- compute(nn, test.data)
sse9=sum(test.data$deaths-nn.results$net.result)^2
c(mean(sse1),mean(sse2),mean(sse3),mean(sse4),mean(sse5),mean(sse6),mean(sse7),mean(sse8),mean(sse9))/(nrow(test.data))
r2 <- rSquared(test.data$deathrate, resid = test.data$deaths-predict(fit5,new=test.data))
test.data$deaths-predict(fit5,new=test.data)
r2 <- rSquared(test.data$deaths, resid = test.data$deaths-predict(fit5,new=test.data))
r2
k=nrow(coef(model2))-1
1-((1-r2)*(n-1))/(n-k-1)
X1=cbind(1,X)
testlassoridge(X1%*%coef(model));
#plot doesent look random
#we received p-value <2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, the residuals do not follow a normal distribution
testlassoridge(X%*%coef(model));
X1
testlassoridge(X1%*%coef(model));
X1%*%coef(model)
testlassoridge
X1%*%coef(model)
testlassoridge
train.data$deathrate-X1%*%coef(model)
#II)
#Can we do a better job modelling the total deaths?
#(REQ: Use of linear regression)
temp2=subset(temp,select=-c(deaths2,deathrate,active))
#we can run same code to deal with spread of outliers
temp2$GDP.2018=I(log(temp2$GDP))
temp2$Population.2020=I(log(temp2$Population.2020))
temp2$Total.tests=I(log(temp2$Total.tests))
temp2$recoveries=I(sqrt(temp2$recoveries))
temp2$cases2=I(sqrt(temp2$cases2))
temp2$cases=I(sqrt(temp2$cases))
boxplot(temp2)
#80/20 split for training and testing data
sample <-sample.int(n =nrow(temp2),size =floor(.80*nrow(temp2)), replace = F)
train.data <- temp2[sample, ]
test.data <-temp2[-sample,]
train.data=drop_na(train.data)
fit=lm(deaths~.,data=train.data)
vif(fit)
#deal with multicollinearity
length=1
while(length!=ncol(train.data)){
length=ncol(train.data)
train.data=dropvif1(fit,train.data)
fit=lm(deaths~.,data=train.data)
}
vif(fit)
#looking good, we have our first model
summary(fit)
plot(fit)
fit2=stepwise(fit,direction='forward/backward',criterion='BIC',trace='false'); summary(fit2)
plot(fit2)
fit3=stepwise(fit,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit3)
fit4=lm(deaths~.^2,data=train.data)
plot(fit4)
#model 5
fit5=stepwise(fit4,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit5)
fit6=stepwise(fit4,direction='forward/backward',criterion='AIC',trace='false'); summary(fit3)
plot(fit6)
#model 7, LASSO Model
X = model.matrix(deaths~.,train.data )[,-1]
Y=train.data$deaths
cv=cv.glmnet(X,Y,alpha =1)
model=glmnet(X,Y,alpha =1, lambda=cv$lambda.min)
X1=cbind(1,X)
testlassoridge(X1%*%coef(model));
#plot doesent look random
#we received p-value <2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, the residuals do not follow a normal distribution
#model 8, Ridge Model
cv=cv.glmnet(X,Y,alpha =0)
model2=glmnet(X,Y,alpha =0, lambda=cv$lambda.min)
testlassoridge(X1%*%coef(model2));
#plot doesent look random
#we received p-value <2.2e-16 in the shapiro wilk test for normality
#meaning that we reject the null hypothesis of no significant difference
#with the normal distribution, and conclude that under the current
#evidence, the residuals do not follow a normal distribution
#model 9, neural net
#we need to normalize the data
ntrain.data=as.data.frame(lapply(train.data,normalize))
nn=neuralnet(deaths ~ .-cases , data=ntrain.data, hidden=c(16,25), linear.output =T, threshold=0.1)
#dropping cases or else get:
#Error: the error derivative contains a NA; varify that the derivative function
#does not divide by 0 (e.g. cross entropy)
#which shouldnt be possible as probability never 0 implying this column led
#R to rounding some really small number
train.data
testlassoridge1=function(fits){
residuals=train.data$deaths-fits
plot(fits[1:length(fits)]~residuals[1:length(residuals)])
return(shapiro.test(residuals[1:length(residuals)]))
}
testlassoridge1(X1%*%coef(model));
testlassoridge1(X1%*%coef(model2));
testlassoridge1(X1%*%coef(model2));
testlassoridge1(X1%*%coef(model));
testlassoridge1(X1%*%coef(model2));
options(warn=-1)
source('~/Harvard/MATH E-23c/Project/project.R')
source('~/Harvard/MATH E-23c/Project/project.R')
testlassoridge1(X1%*%coef(model2));
(fits=X1%*%coef(model2))
fits=X1%*%coef(model2)
ggplot(aes(x = fits[1:length(fits)], y = residuals[1:length(residuals)])+ geom_point(color = "cyan", size = 2, shape = 23)
)
ggplot(aes(x = fits[1:length(fits)], y = residuals[1:length(residuals)]))+ geom_point(color = "cyan", size = 2, shape = 23)
rlang::last_error()
